# version: "3.9"

# services:
#   # --- 1. HDFS (Storage) ---
#   namenode:
#     image: bde2020/hadoop-namenode:2.0.0-hadoop${HADOOP_VERSION}-java8
#     container_name: namenode
#     ports:
#       - "9870:9870"
#       - "9000:9000"
#     environment:
#       - CLUSTER_NAME=test
#       - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#       - TZ=UTC
#     volumes:
#       - hadoop_namenode:/hadoop/dfs/name
#     healthcheck:
#       test: ["CMD", "curl", "-f", "http://localhost:9870"]
#       interval: 10s
#       timeout: 5s
#       retries: 30
#       start_period: 30s   # gives time for first format
#     networks:
#       - hadoop

#   datanode:
#     image: bde2020/hadoop-datanode:2.0.0-hadoop${HADOOP_VERSION}-java8
#     container_name: datanode
#     depends_on:
#       - namenode
#     environment:
#       - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#       - TZ=UTC
#     volumes:
#       - hadoop_datanode:/hadoop/dfs/data
#     networks:
#       - hadoop

#   # --- 2. YARN (MapReduce) ---
#   resourcemanager:
#     image: bde2020/hadoop-resourcemanager:2.0.0-hadoop${HADOOP_VERSION}-java8
#     container_name: resourcemanager
#     ports:
#       - "8088:8088" # YARN Web UI
#     environment:
#       - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#       - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager   # ← ADD THIS
#       - TZ=UTC
#     depends_on:
#       - namenode
#       - datanode
#     networks:
#       - hadoop

#   nodemanager:
#     image: bde2020/hadoop-nodemanager:2.0.0-hadoop${HADOOP_VERSION}-java8
#     container_name: nodemanager
#     environment:
#       - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#       - YARN_CONF_yarn_resourcemanager_hostname=resourcemanager   # ← ADD THIS
#       - TZ=UTC
#     depends_on:
#       - namenode
#       - datanode
#       - resourcemanager
#     networks:
#       - hadoop

#   # --- 3. Kafka (Message Broker for Real-time Ingestion) ---
#   zookeeper:
#     image: confluentinc/cp-zookeeper:7.4.0
#     container_name: zookeeper
#     environment:
#       - ZOOKEEPER_CLIENT_PORT=2181
#       - ZOOKEEPER_TICK_TIME=2000
#       - TZ=UTC
#     networks:
#       - hadoop

#   kafka:
#     image: confluentinc/cp-kafka:7.4.0
#     container_name: kafka
#     depends_on:
#       - zookeeper
#     ports:
#       - "9092:9092"
#       - "9093:9093"
#     environment:
#       - KAFKA_BROKER_ID=1
#       - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
#       - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9093
#       - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
#       - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
#       - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
#       - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
#       - TZ=UTC
#     networks:
#       - hadoop

#   # --- 4. Spark ---
#   spark-master:
#     image: bde2020/spark-master:3.1.1-hadoop3.2
#     container_name: spark-master
#     ports:
#       - "8080:8080" # Spark Master Web UI
#       - "7077:7077" # Spark submit port
#     environment:
#       - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#     depends_on:
#       - namenode
#     networks:
#       - hadoop

#   spark-worker:
#     image: bde2020/spark-worker:3.1.1-hadoop3.2
#     container_name: spark-worker
#     depends_on:
#       - spark-master
#     environment:
#       - SPARK_MASTER_URL=spark://spark-master:7077
#       - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#     networks:
#       - hadoop

#   hive-metastore:
#     image: bde2020/hive:2.3.2-postgresql-metastore
#     container_name: hive-metastore
#     command: /opt/hive/bin/hive --service metastore
#     environment:
#       - SERVICE_PREPEND_PATH=/opt/hive/bin
#       - HIVE_METASTORE_DB_URL=jdbc:postgresql://hive-metastore-db:5432/metastore
#       - HIVE_METASTORE_DB_USER=hive
#       - HIVE_METASTORE_DB_PASSWORD=hive
#       - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#     depends_on:
#       namenode:
#         condition: service_healthy
#       hive-metastore-db:
#         condition: service_healthy
#     ports:
#       - "9083:9083"   # metastore thrift port (useful for debugging)
#     healthcheck:
#       test: ["CMD", "nc", "-z localhost", "9083"]
#       interval: 10s
#       timeout: 5s
#       retries: 10
#     restart: unless-stopped
#     networks:
#       - hadoop

#   hive-metastore-init:
#     image: bde2020/hive:2.3.2-postgresql-metastore
#     depends_on:
#       hive-metastore-db:
#         condition: service_healthy
#       hive-metastore:
#         condition: service_healthy
#     entrypoint: /opt/hive/bin/schematool
#     command: -dbType postgres -initSchema
#     networks:
#       - hadoop
#   # --- 5. Hive ---
#   hive-server:
#     image: bde2020/hive:2.3.2-postgresql-metastore
#     container_name: hive-server
#     ports:
#       - "10000:10000" # Hive JDBC port
#     environment:
#       - HIVE_METASTORE_DB_URL=jdbc:postgresql://hive-metastore-db:5432/metastore
#       - HIVE_METASTORE_DB_USER=hive
#       - HIVE_METASTORE_DB_PASSWORD=hive
#       - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#       - SERVICE_OPTS=-Dhive.metastore.failure.retries=20 -Dhive.metastore.connect.retries=20
#     depends_on:
#       namenode:
#         condition: service_healthy   # ← ALSO HERE
#       hive-metastore:
#         condition: service_started
#     networks:
#       - hadoop

#   hive-metastore-db:
#     image: postgres:12
#     container_name: hive-metastore-db
#     environment:
#       - POSTGRES_USER=hive
#       - POSTGRES_PASSWORD=hive
#       - POSTGRES_DB=metastore          # This creates the DB on first start
#     healthcheck:
#       test: ["CMD-SHELL", "pg_isready -U hive -d metastore -h localhost"]
#       interval: 5s
#       timeout: 5s
#       retries: 12
#       start_period: 10s
#     volumes:
#       - hive_metastore_db:/var/lib/postgresql/data
#     networks:
#       - hadoop

#   # --- 6. Zeppelin (Your UI) ---
#   zeppelin:
#     image: apache/zeppelin:${ZEPPELIN_VERSION}
#     container_name: zeppelin
#     ports:
#       - "9090:8080" # Zeppelin Web UI (local 9090 -> container 8080)
#     depends_on:
#       - spark-master
#       - hive-server
#     volumes:
#       - ./data:/data # Map a local 'data' folder for your datasets
#       - zeppelin_notebooks:/opt/zeppelin/notebook
#       - zeppelin_logs:/opt/zeppelin/logs
#     environment:
#       - SPARK_MASTER=spark://spark-master:7077
#       - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
#     networks:
#       - hadoop

#   # --- 7. ClickHouse (Data Warehouse) ---
#   clickhouse:
#     image: clickhouse/clickhouse-server:latest
#     container_name: clickhouse
#     ports:
#       - "8123:8123"
#       - "9001:9000" # Native TCP interface (changed from 9000 to avoid conflict with HDFS)
#     environment:
#       - CLICKHOUSE_DB=weather_analytics
#       - CLICKHOUSE_USER=default
#       - CLICKHOUSE_PASSWORD=clickhouse123
#       - TZ=UTC
#     volumes:
#       - clickhouse_data:/var/lib/clickhouse
#       - ./src/clickhouse:/docker-entrypoint-initdb.d
#     networks:
#       - hadoop

# # --- Networks ---
# networks:
#   hadoop:
#     driver: bridge

# # --- Volumes (to persist data) ---
# volumes:
#   hadoop_namenode:
#   hadoop_datanode:
#   hive_metastore_db:
#   zeppelin_notebooks:
#   zeppelin_logs:
#   clickhouse_data:



version: "3.9"

services:
  # ──────────────────────────────────────────────────────────────
  # 1. HDFS
  # ──────────────────────────────────────────────────────────────
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop${HADOOP_VERSION:-3.2.1}-java8
    container_name: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      CLUSTER_NAME: test
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      TZ: UTC
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9870"]
      interval: 10s
      timeout: 5s
      retries: 30
      start_period: 30s
    networks: [hadoop]

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop${HADOOP_VERSION:-3.2.1}-java8
    depends_on: [namenode]
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      TZ: UTC
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    networks: [hadoop]

  # ──────────────────────────────────────────────────────────────
  # 2. YARN
  # ──────────────────────────────────────────────────────────────
  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop${HADOOP_VERSION:-3.2.1}-java8
    container_name: resourcemanager
    ports:
      - "8088:8088"
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      YARN_CONF_yarn_resourcemanager_hostname: resourcemanager   # ← critical
      TZ: UTC
    depends_on: [namenode, datanode]
    networks: [hadoop]

  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop${HADOOP_VERSION:-3.2.1}-java8
    container_name: nodemanager
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      YARN_CONF_yarn_resourcemanager_hostname: resourcemanager   # ← critical
      TZ: UTC
    depends_on: [namenode, datanode, resourcemanager]
    networks: [hadoop]

  # ──────────────────────────────────────────────────────────────
  # 3. Kafka + Zookeeper
  # ──────────────────────────────────────────────────────────────
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      TZ: UTC
    networks: [hadoop]

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on: [zookeeper]
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      TZ: UTC
    networks: [hadoop]

  # ──────────────────────────────────────────────────────────────
  # 4. Spark (Custom images with Python ML dependencies)
  # ──────────────────────────────────────────────────────────────
  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    image: custom-spark-master:3.1.1-hadoop3.2
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    networks: [hadoop]

  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark-worker
    image: custom-spark-worker:3.1.1-hadoop3.2
    depends_on: [spark-master]
    environment:
      SPARK_MASTER_URL: spark://spark-master:7077
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
    networks: [hadoop]

  # ──────────────────────────────────────────────────────────────
  # 5. Hive Metastore + DB + Init
  # ──────────────────────────────────────────────────────────────
  hive-metastore-db:
    image: postgres:12
    container_name: hive-metastore-db
    environment:
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive
      POSTGRES_DB: metastore
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 5s
      timeout: 5s
      retries: 12
      start_period: 10s
    volumes:
      - hive_metastore_db:/var/lib/postgresql/data
    networks: [hadoop]

  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    command: /opt/hive/bin/hive --service metastore
    environment:
      - SERVICE_PREPEND_PATH=/opt/hive/bin
      - HIVE_METASTORE_DB_URL=jdbc:postgresql://hive-metastore-db:5432/metastore
      - HIVE_METASTORE_DB_USER=hive
      - HIVE_METASTORE_DB_PASSWORD=hive
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HIVE_SITE_CONF_datanucleus_schema_autoCreateAll=true
      - HIVE_SITE_CONF_hive_metastore_schema_verification=false
    depends_on:
      hive-metastore-db:
        condition: service_healthy
      namenode:
        condition: service_healthy
    ports:
      - "9083:9083"
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9083"]
      interval: 10s
      timeout: 5s
      retries: 30          # ← increased from 10 → 30
      start_period: 60s     # ← THIS IS THE KEY: give it 60 seconds grace period
    restart: unless-stopped
    networks: [hadoop]

  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    ports:
      - "10000:10000"
      - "10002:10002"   # Beeline
    environment:
      - HIVE_SITE_CONF_hive_metastore_uris=thrift://hive-metastore:9083
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
    depends_on:
      namenode:
        condition: service_healthy
      hive-metastore:
        condition: service_healthy
    networks: [hadoop]

  # ──────────────────────────────────────────────────────────────
  # 6. Zeppelin
  # ──────────────────────────────────────────────────────────────
  zeppelin:
    image: apache/zeppelin:${ZEPPELIN_VERSION:-0.10.1}
    container_name: zeppelin
    ports:
      - "9090:8080"
    environment:
      SPARK_MASTER: spark://spark-master:7077
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      ZEPPELIN_PORT: 8080
      MASTER: spark://spark-master:7077
      SPARK_SUBMIT_OPTIONS: "--conf spark.driver.memory=2g --conf spark.executor.memory=2g"
    volumes:
      - ./data:/data
      - zeppelin_notebooks:/opt/zeppelin/notebook
      - zeppelin_logs:/opt/zeppelin/logs
      - ./scripts:/scripts
    depends_on:
      - spark-master
      - hive-server
    networks: [hadoop]

  # ──────────────────────────────────────────────────────────────
  # 7. ClickHouse
  # ──────────────────────────────────────────────────────────────
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9001:9000"
    environment:
      - CLICKHOUSE_DB=weather_analytics
      - CLICKHOUSE_USER=default
      - CLICKHOUSE_PASSWORD=
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
    ulimits:
      nofile: { soft: 262144, hard: 262144 }
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./src/clickhouse:/docker-entrypoint-initdb.d
    networks: [hadoop]

networks:
  hadoop:
    driver: bridge

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hive_metastore_db:
  zeppelin_notebooks:
  zeppelin_logs:
  clickhouse_data: